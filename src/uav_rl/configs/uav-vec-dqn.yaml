train:
  load_weights: True
  offline: False
env:
  env: UavEnvVec
  is_atari: False
  env_config:
    dimensions: !!python/tuple [ 1000, 1000 ]
    fixed_obstacles: 20
    dynamic_obstacles: 20
    occur_obstacles: 1
    occur_number_max: 3
    return_raster: False
    prevent_stiff: False
    use_lidar: True
    draw_lidar: False
    lidar_range: 250.0
    lidar_rays: 42
    field_of_view: 210.0
    center_obstacles: False
model:
  fcnet_hiddens: [ 256 ]
  use_inverse_dynamic: False
  channels_last: True
  encoder_config: [
    {
      model_name: 'cnn',
      in_keys: [ 'raster' ],
      out_keys: [ 'logits' ],
      model_config: {
        num_channels: [ 16, 32, 64, 64 ],
        kernel_sizes: 3,
        activation_class: 'relu',
        squash_last_layer: True,
      },
    },
    {
      model_name: 'mlp',
      in_keys: [ 'logits', 'observation' ],
      out_keys: [ 'logits' ],
      model_config: {
        out_features: 256,
        activation_class: 'relu',
        activate_last_layer: True,
      },
    },
  ]
algo:
  name: DQN
  training:
    lr: 1.0e-4
    gamma: 0.99
    train_batch_size: 512
#    training_intensity: 5.0
    double_q: True
    dueling: True
    n_step: 1
    hiddens: [ 256 ]
    num_steps_sampled_before_learning_starts: 5_000
    target_network_update_freq: 400
#    target_network_update_freq: 1
#    tau: 0.997
    replay_buffer_config:
      type: MultiAgentPrioritizedReplayBuffer
      capacity: 500000
      prioritized_replay_alpha: 0.6
      prioritized_replay_beta: 0.4
      prioritized_replay_eps: 3.0e-6
stop:
  #  training_iteration: 10_000
  timesteps_total: 1_000_000
#  episode_reward_mean: 1000.0
